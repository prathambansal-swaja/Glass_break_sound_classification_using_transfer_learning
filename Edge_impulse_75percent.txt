import sys
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import (
    Input,
    Reshape,
    Conv2D,
    BatchNormalization,
    ReLU,
    MaxPooling2D,
    AveragePooling2D,
    Flatten,
    Dense
)
from tensorflow.keras.optimizers.legacy import Nadam

sys.path.append('./resources/libraries')
import ei_tensorflow.training

# =========================================================
# CONFIG (MUST MATCH EI DSP OUTPUT)
# =========================================================
INPUT_FRAMES = 40
INPUT_BINS   = 40
INPUT_SIZE   = INPUT_FRAMES * INPUT_BINS  # 1600

NUM_CLASSES = 2
EPOCHS = args.epochs or 30
LEARNING_RATE = args.learning_rate or 0.001
BATCH_SIZE = args.batch_size or 32
ENSURE_DETERMINISM = args.ensure_determinism

# =========================================================
# MODEL (EI + NDP120 SAFE)
# =========================================================
inputs = Input(shape=(INPUT_SIZE,), name="ei_input")
x = Reshape((INPUT_FRAMES, INPUT_BINS, 1))(inputs)

x = Conv2D(16, (3, 3), padding="same", use_bias=False, name="conv1")(x)
x = BatchNormalization(name="bn1")(x)
x = ReLU(name="relu1")(x)
x = MaxPooling2D((2, 2), name="pool1")(x)

x = Conv2D(32, (3, 3), padding="same", use_bias=False, name="conv2")(x)
x = BatchNormalization(name="bn2")(x)
x = ReLU(name="relu2")(x)
x = MaxPooling2D((2, 2), name="pool2")(x)

x = Conv2D(48, (3, 3), padding="same", use_bias=False, name="conv3")(x)
x = BatchNormalization(name="bn3")(x)
x = ReLU(name="relu3")(x)

# Syntiant-friendly global pooling
x = AveragePooling2D(pool_size=(x.shape[1], x.shape[2]))(x)
x = Flatten()(x)

x = Dense(64, activation="relu", use_bias=False, name="fc1")(x)
outputs = Dense(NUM_CLASSES, activation="softmax", name="output")(x)

model = Model(inputs, outputs)
model.summary()

# =========================================================
# DATA PIPELINE
# =========================================================
if not ENSURE_DETERMINISM:
    train_dataset = train_dataset.shuffle(buffer_size=BATCH_SIZE * 4)

prefetch_policy = 1 if ENSURE_DETERMINISM else tf.data.AUTOTUNE

train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(prefetch_policy)
validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(prefetch_policy)

# =========================================================
# TRAINING
# =========================================================
model.compile(
    optimizer=Nadam(learning_rate=LEARNING_RATE),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=EPOCHS,
    verbose=2,
    callbacks=callbacks,
    class_weight=ei_tensorflow.training.get_class_weights(Y_train)
)
