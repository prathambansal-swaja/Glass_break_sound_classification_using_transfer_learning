import sys
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import (
    Input,
    Reshape,
    Conv2D,
    BatchNormalization,
    ReLU,
    MaxPooling2D,
    AveragePooling2D,
    Flatten,
    Dense
)
from tensorflow.keras.optimizers.legacy import Nadam

sys.path.append('./resources/libraries')
import ei_tensorflow.training

# =========================================================
# CONFIG (MUST MATCH EI DSP OUTPUT)
# =========================================================
INPUT_FRAMES = 40
INPUT_BINS   = 40
INPUT_SIZE   = INPUT_FRAMES * INPUT_BINS  # 1600

NUM_CLASSES = 2
EPOCHS = args.epochs or 50
LEARNING_RATE = args.learning_rate or 0.001
BATCH_SIZE = args.batch_size or 32
ENSURE_DETERMINISM = args.ensure_determinism

# =========================================================
# MODEL (EI + NDP120 SAFE)
# =========================================================
#inputs = Input(shape=(INPUT_SIZE,), name="ei_input")
#inputs = Input(shape=(INPUT_SIZE,), batch_size=None, name="ei_input")
#inputs = Input(batch_shape=(None, INPUT_SIZE), name="ei_input")

#x = Reshape((INPUT_FRAMES, INPUT_BINS, 1))(inputs)

#x = Conv2D(16, (3, 3), padding="same", use_bias=False, name="conv1")(x)
#x = BatchNormalization(name="bn1")(x)
#x = ReLU(name="relu1")(x)
#x = MaxPooling2D((2, 2), name="pool1")(x)

#x = Conv2D(32, (3, 3), padding="same", use_bias=False, name="conv2")(x)
#x = BatchNormalization(name="bn2")(x)
#x = ReLU(name="relu2")(x)
#x = MaxPooling2D((2, 2), name="pool2")(x)

#x = Conv2D(48, (3, 3), padding="same", use_bias=False, name="conv3")(x)
#x = BatchNormalization(name="bn3")(x)
#x = ReLU(name="relu3")(x)

# Syntiant-friendly global pooling
#x = AveragePooling2D(pool_size=(x.shape[1], x.shape[2]))(x)
#x = Flatten()(x)

#x = Dense(64, activation="relu", use_bias=False, name="fc1")(x)
#outputs = Dense(NUM_CLASSES, activation="softmax", name="output")(x)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer

model = Sequential(name="ei_model")

model.add(InputLayer(input_shape=(INPUT_SIZE,), name="ei_input"))
model.add(Reshape((INPUT_FRAMES, INPUT_BINS, 1)))

model.add(Conv2D(16, (3,3), padding="same", use_bias=False, name="conv1"))
model.add(BatchNormalization(name="bn1"))
model.add(ReLU(name="relu1"))
model.add(MaxPooling2D((2,2), name="pool1"))

model.add(Conv2D(32, (3,3), padding="same", use_bias=False, name="conv2"))
model.add(BatchNormalization(name="bn2"))
model.add(ReLU(name="relu2"))
model.add(MaxPooling2D((2,2), name="pool2"))

model.add(Conv2D(48, (3,3), padding="same", use_bias=False, name="conv3"))
model.add(BatchNormalization(name="bn3"))
model.add(ReLU(name="relu3"))

model.add(AveragePooling2D(pool_size=(10,10)))
model.add(Flatten())

model.add(Dense(64, activation="relu", use_bias=False, name="fc1"))
model.add(Dense(NUM_CLASSES, activation="softmax", name="output"))

#model = Model(inputs, outputs)
model.summary()

# =========================================================
# DATA PIPELINE
# =========================================================
if not ENSURE_DETERMINISM:
    train_dataset = train_dataset.shuffle(buffer_size=BATCH_SIZE * 4)

prefetch_policy = 1 if ENSURE_DETERMINISM else tf.data.AUTOTUNE

train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(prefetch_policy)
validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(prefetch_policy)

# =========================================================
# TRAINING
# =========================================================
model.compile(
    optimizer=Nadam(learning_rate=LEARNING_RATE),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=EPOCHS,
    verbose=2,
    callbacks=callbacks,
    class_weight=ei_tensorflow.training.get_class_weights(Y_train)
)
